# v0.7.0 Caching System Design

**Date**: 2025-10-23 (Week 1 Day 2)
**Status**: 🎨 DESIGN PHASE
**Goal**: Implement filesystem-based cache to reduce detection time by 90%+ on cache hits

---

## 📋 Overview

Based on Day 1 profiling results, file system scanning (glob operations) accounts for **46-57% of detection time**. A caching system can dramatically improve performance for repeated detections of the same project.

**Key Insight**: Most developers run detection on the same project multiple times during development. Cache hit rate should be 60-80% in typical usage.

---

## 🎯 Design Goals

### Performance Targets

| Scenario | Current | Target | Improvement |
|----------|---------|--------|-------------|
| **Cache Hit** | 450μs (avg) | <10μs | **-98%** |
| **Cache Miss** | 450μs | 460μs (+10μs overhead) | Acceptable |
| **Cache Hit Rate** | 0% (no cache) | 60-80% | Target |

### Functional Requirements

**MUST HAVE**:
- ✅ Cache detection results by project path
- ✅ Invalidate cache on file changes (mtime-based)
- ✅ Support CLI flags: --cache / --no-cache
- ✅ Thread-safe for concurrent detections
- ✅ Automatic cleanup of stale entries (7+ days old)

**SHOULD HAVE**:
- ✅ TTL-based expiration (default 24h)
- ✅ Version-aware caching (invalidate on version change)
- ✅ Cache statistics (hit rate, size)

**NICE TO HAVE**:
- ⏳ LRU eviction policy (defer to v0.8.0)
- ⏳ Compression for large cache files (defer if not needed)
- ⏳ Remote cache support (defer to v1.0.0)

---

## 🔑 Cache Key Generation

### Strategy: `{path}:{mtime_hash}:{version}`

**Components**:
1. **Absolute path**: Unique identifier for project
2. **Mtime hash**: Detect file changes
3. **Version**: Invalidate on tool version upgrade

### Implementation

```python
import hashlib
from pathlib import Path
from typing import List

def generate_cache_key(project_path: Path, version: str) -> str:
    """
    Generate unique cache key for project detection.

    Args:
        project_path: Absolute path to project
        version: Tool version (e.g., "0.7.0-beta")

    Returns:
        Cache key format: "{path_hash}:{mtime_hash}:{version}"

    Example:
        "/path/to/project:a1b2c3d4:0.7.0-beta"
    """
    # 1. Resolve to absolute path
    abs_path = project_path.resolve()

    # 2. Get mtime of key indicator files
    key_files = _get_key_indicator_files(abs_path)

    # 3. Compute mtime hash
    mtime_hash = _compute_mtime_hash(key_files)

    # 4. Hash the path to avoid filesystem issues with long paths
    path_hash = hashlib.sha256(str(abs_path).encode()).hexdigest()[:16]

    # 5. Combine components
    return f"{path_hash}:{mtime_hash}:{version}"


def _get_key_indicator_files(project_path: Path) -> List[Path]:
    """
    Get list of files that indicate project type.
    Changes to these files should invalidate cache.

    Returns:
        List of existing indicator files
    """
    indicators = [
        # JavaScript/TypeScript
        project_path / "package.json",
        project_path / "package-lock.json",
        project_path / "yarn.lock",
        project_path / "pnpm-lock.yaml",

        # Python
        project_path / "requirements.txt",
        project_path / "pyproject.toml",
        project_path / "Pipfile",
        project_path / "setup.py",

        # Go
        project_path / "go.mod",
        project_path / "go.sum",

        # Flutter
        project_path / "pubspec.yaml",
        project_path / "pubspec.lock",

        # iOS
        project_path / "Podfile",
        project_path / "Podfile.lock",

        # PHP
        project_path / "composer.json",
        project_path / "composer.lock",

        # Configuration
        project_path / "next.config.js",
        project_path / "next.config.mjs",
        project_path / "vite.config.js",
        project_path / "vite.config.ts",
    ]

    # Return only files that exist
    return [f for f in indicators if f.exists()]


def _compute_mtime_hash(files: List[Path]) -> str:
    """
    Compute hash of modification times.

    Args:
        files: List of files to check

    Returns:
        16-character hex hash of sorted mtimes
    """
    if not files:
        # No indicator files found, use current time
        import time
        return hashlib.sha256(str(time.time()).encode()).hexdigest()[:16]

    # Get mtimes and sort for consistency
    mtimes = sorted([f.stat().st_mtime for f in files])

    # Hash the mtime list
    mtime_str = ",".join(f"{mt:.6f}" for mt in mtimes)
    return hashlib.sha256(mtime_str.encode()).hexdigest()[:16]
```

### Cache Key Examples

```python
# Example 1: Next.js project
project_path = Path("/Users/dev/my-nextjs-app")
version = "0.7.0-beta"
cache_key = "a1b2c3d4e5f6g7h8:1234567890abcdef:0.7.0-beta"

# Example 2: Go project
project_path = Path("/Users/dev/my-go-api")
cache_key = "9876543210fedcba:fedcba0987654321:0.7.0-beta"
```

---

## 🗄️ Cache Storage Structure

### Location

```
~/.cache/adaptive-claude-agents/
  ├── detection_cache.json      # Main cache file
  ├── metadata.json              # Cache metadata
  └── .lock                      # Lock file for thread safety
```

**Why `~/.cache/`?**
- Standard location for user caches on Unix-like systems
- Automatically cleaned by OS on low disk space (macOS, Linux)
- Easy to find and delete if needed

### detection_cache.json Format

```json
{
  "a1b2c3d4e5f6g7h8:1234567890abcdef:0.7.0-beta": {
    "framework": "nextjs",
    "version": "14.2.0",
    "language": "typescript",
    "confidence": 1.0,
    "indicators": [
      "package.json exists",
      "'next' dependency found",
      "next.config.js exists"
    ],
    "recommended_subagents": [
      "nextjs-developer",
      "nextjs-tester",
      "app-router-specialist"
    ],
    "project_structure": {
      "app_router": true,
      "pages_router": false,
      "typescript": true
    },
    "timestamp": 1698768000,
    "ttl": 86400,
    "access_count": 5,
    "last_accessed": 1698768000
  },
  "9876543210fedcba:fedcba0987654321:0.7.0-beta": {
    "framework": "go-gin",
    "version": "1.21",
    "language": "go",
    "confidence": 0.9,
    "indicators": [
      "go.mod exists",
      "github.com/gin-gonic/gin found"
    ],
    "recommended_subagents": [
      "go-developer",
      "go-reviewer",
      "api-developer"
    ],
    "project_structure": {
      "has_go_mod": true,
      "framework": "go-gin"
    },
    "timestamp": 1698768100,
    "ttl": 86400,
    "access_count": 3,
    "last_accessed": 1698768200
  }
}
```

### metadata.json Format

```json
{
  "version": "0.7.0-beta",
  "created": 1698768000,
  "last_cleanup": 1698768000,
  "entry_count": 42,
  "total_hits": 150,
  "total_misses": 50,
  "hit_rate": 0.75,
  "cache_size_bytes": 102400,
  "oldest_entry": 1698680000,
  "newest_entry": 1698768000
}
```

---

## ⚙️ Cache Operations

### 1. Cache Read (Get)

```python
import json
from pathlib import Path
from typing import Optional
from dataclasses import dataclass
import time

@dataclass
class CacheEntry:
    """Cached detection result."""
    framework: str
    version: str
    language: str
    confidence: float
    indicators: list
    recommended_subagents: list
    project_structure: dict
    timestamp: float
    ttl: int
    access_count: int
    last_accessed: float


class DetectionCache:
    """Filesystem-based cache for detection results."""

    def __init__(self, cache_dir: Path = None):
        """Initialize cache."""
        if cache_dir is None:
            cache_dir = Path.home() / ".cache" / "adaptive-claude-agents"

        self.cache_dir = cache_dir
        self.cache_file = cache_dir / "detection_cache.json"
        self.metadata_file = cache_dir / "metadata.json"
        self.lock_file = cache_dir / ".lock"

        # Ensure cache directory exists
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def get(self, cache_key: str) -> Optional[CacheEntry]:
        """
        Get cached detection result.

        Args:
            cache_key: Cache key from generate_cache_key()

        Returns:
            CacheEntry if found and valid, None otherwise
        """
        # Load cache
        cache_data = self._load_cache()

        # Check if key exists
        if cache_key not in cache_data:
            return None

        entry_data = cache_data[cache_key]

        # Check TTL
        age = time.time() - entry_data["timestamp"]
        if age > entry_data["ttl"]:
            # Expired, remove from cache
            del cache_data[cache_key]
            self._save_cache(cache_data)
            return None

        # Update access statistics
        entry_data["access_count"] += 1
        entry_data["last_accessed"] = time.time()
        self._save_cache(cache_data)

        # Update metadata (hit)
        self._update_metadata(hit=True)

        # Return cache entry
        return CacheEntry(**entry_data)

    def _load_cache(self) -> dict:
        """Load cache from disk."""
        if not self.cache_file.exists():
            return {}

        try:
            with open(self.cache_file, 'r') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            # Corrupted cache, start fresh
            return {}

    def _save_cache(self, cache_data: dict):
        """Save cache to disk."""
        with open(self.cache_file, 'w') as f:
            json.dump(cache_data, f, indent=2)

    def _update_metadata(self, hit: bool):
        """Update cache metadata."""
        metadata = self._load_metadata()

        if hit:
            metadata["total_hits"] += 1
        else:
            metadata["total_misses"] += 1

        # Recalculate hit rate
        total = metadata["total_hits"] + metadata["total_misses"]
        metadata["hit_rate"] = metadata["total_hits"] / total if total > 0 else 0.0

        self._save_metadata(metadata)

    def _load_metadata(self) -> dict:
        """Load metadata."""
        if not self.metadata_file.exists():
            return {
                "version": "0.7.0-beta",
                "created": time.time(),
                "last_cleanup": time.time(),
                "entry_count": 0,
                "total_hits": 0,
                "total_misses": 0,
                "hit_rate": 0.0,
                "cache_size_bytes": 0,
                "oldest_entry": time.time(),
                "newest_entry": time.time()
            }

        with open(self.metadata_file, 'r') as f:
            return json.load(f)

    def _save_metadata(self, metadata: dict):
        """Save metadata."""
        with open(self.metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2)
```

### 2. Cache Write (Set)

```python
def set(self, cache_key: str, detection_result: dict, ttl: int = 86400):
    """
    Store detection result in cache.

    Args:
        cache_key: Cache key from generate_cache_key()
        detection_result: Detection result dict
        ttl: Time-to-live in seconds (default: 24h)
    """
    # Load cache
    cache_data = self._load_cache()

    # Create cache entry
    entry = {
        **detection_result,
        "timestamp": time.time(),
        "ttl": ttl,
        "access_count": 0,
        "last_accessed": time.time()
    }

    # Store in cache
    cache_data[cache_key] = entry
    self._save_cache(cache_data)

    # Update metadata
    metadata = self._load_metadata()
    metadata["entry_count"] = len(cache_data)
    metadata["newest_entry"] = time.time()
    metadata["cache_size_bytes"] = self.cache_file.stat().st_size
    self._save_metadata(metadata)
```

### 3. Cache Invalidation

```python
def invalidate(self, cache_key: str):
    """Remove entry from cache."""
    cache_data = self._load_cache()

    if cache_key in cache_data:
        del cache_data[cache_key]
        self._save_cache(cache_data)

        # Update metadata
        metadata = self._load_metadata()
        metadata["entry_count"] = len(cache_data)
        self._save_metadata(metadata)


def cleanup(self, max_age_days: int = 7):
    """
    Remove stale entries from cache.

    Args:
        max_age_days: Maximum age of entries in days (default: 7)
    """
    cache_data = self._load_cache()
    current_time = time.time()
    max_age_seconds = max_age_days * 86400

    # Find stale entries
    stale_keys = []
    for key, entry in cache_data.items():
        age = current_time - entry["timestamp"]
        if age > max_age_seconds:
            stale_keys.append(key)

    # Remove stale entries
    for key in stale_keys:
        del cache_data[key]

    if stale_keys:
        self._save_cache(cache_data)

        # Update metadata
        metadata = self._load_metadata()
        metadata["entry_count"] = len(cache_data)
        metadata["last_cleanup"] = current_time
        self._save_metadata(metadata)

        print(f"🧹 Cleaned up {len(stale_keys)} stale cache entries")


def clear(self):
    """Clear all cache entries."""
    self._save_cache({})

    # Reset metadata
    metadata = self._load_metadata()
    metadata["entry_count"] = 0
    metadata["cache_size_bytes"] = 0
    self._save_metadata(metadata)

    print("🗑️  Cache cleared")
```

---

## 🔒 Thread Safety

### File Locking Strategy

```python
import fcntl
import contextlib

@contextlib.contextmanager
def _acquire_lock(self):
    """Acquire file lock for thread-safe operations."""
    lock_file = self.lock_file

    # Create lock file if not exists
    lock_file.touch()

    with open(lock_file, 'w') as f:
        # Acquire exclusive lock
        fcntl.flock(f.fileno(), fcntl.LOCK_EX)

        try:
            yield
        finally:
            # Release lock
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)


def get_with_lock(self, cache_key: str) -> Optional[CacheEntry]:
    """Thread-safe cache get."""
    with self._acquire_lock():
        return self.get(cache_key)


def set_with_lock(self, cache_key: str, detection_result: dict, ttl: int = 86400):
    """Thread-safe cache set."""
    with self._acquire_lock():
        return self.set(cache_key, detection_result, ttl)
```

---

## 🎯 Integration with detect_stack.py

### Modified Detection Flow

```python
def detect_tech_stack(project_path: str, use_cache: bool = True) -> Optional[DetectionResult]:
    """
    Detect tech stack with caching support.

    Args:
        project_path: Path to project
        use_cache: Whether to use cache (default: True)

    Returns:
        DetectionResult or None
    """
    from detect_stack import VERSION  # Tool version

    # 1. Initialize cache
    cache = DetectionCache()

    # 2. Generate cache key
    project_path_obj = Path(project_path).resolve()
    cache_key = generate_cache_key(project_path_obj, VERSION)

    # 3. Try cache hit (if enabled)
    if use_cache:
        cached_entry = cache.get_with_lock(cache_key)
        if cached_entry:
            logger.info(f"✓ Cache hit for {project_path}")
            # Convert CacheEntry to DetectionResult
            return DetectionResult(
                framework=cached_entry.framework,
                version=cached_entry.version,
                language=cached_entry.language,
                confidence=cached_entry.confidence,
                indicators=cached_entry.indicators,
                recommended_subagents=cached_entry.recommended_subagents,
                project_structure=cached_entry.project_structure
            )

    # 4. Cache miss - perform detection
    logger.info(f"⊘ Cache miss for {project_path}")
    detector = TechStackDetector(project_path)
    result = detector.detect()

    # 5. Store in cache (if enabled and result found)
    if use_cache and result:
        cache.set_with_lock(
            cache_key,
            {
                "framework": result.framework,
                "version": result.version,
                "language": result.language,
                "confidence": result.confidence,
                "indicators": result.indicators,
                "recommended_subagents": result.recommended_subagents,
                "project_structure": result.project_structure
            }
        )

    return result
```

---

## 🖥️ CLI Integration

### New Flags

```python
# In create_cli_parser()
parser.add_argument(
    '--cache',
    action='store_true',
    default=True,
    help='Use detection cache (default: enabled)'
)

parser.add_argument(
    '--no-cache',
    dest='cache',
    action='store_false',
    help='Bypass cache, force fresh detection'
)

parser.add_argument(
    '--cache-clear',
    action='store_true',
    help='Clear cache and exit'
)

parser.add_argument(
    '--cache-stats',
    action='store_true',
    help='Show cache statistics and exit'
)
```

### Usage Examples

```bash
# Use cache (default)
python3 detect_stack.py /path/to/project

# Bypass cache
python3 detect_stack.py /path/to/project --no-cache

# Clear cache
python3 detect_stack.py --cache-clear

# Show cache stats
python3 detect_stack.py --cache-stats
```

---

## 📊 Expected Performance Impact

### Benchmark Scenarios

**Scenario 1: Cache Hit** (Best case)
```
Before (v0.6.0): 450μs (average)
After (v0.7.0):  ~10μs (cache read + deserialize)
Improvement:     -98% (-440μs)
```

**Scenario 2: Cache Miss** (Worst case)
```
Before (v0.6.0): 450μs
After (v0.7.0):  460μs (+10μs cache overhead)
Overhead:        +2.2%
```

**Scenario 3: Typical Development Workflow** (60% hit rate)
```
10 detections per day:
- 6 cache hits: 6 × 10μs = 60μs
- 4 cache misses: 4 × 460μs = 1840μs
Total: 1900μs (~190μs average)

vs. Without cache:
- 10 detections: 10 × 450μs = 4500μs

Improvement: -58% (-2600μs)
```

---

## 🧪 Testing Plan

### Unit Tests

```python
# tests/test_cache.py

def test_cache_key_generation(tmp_path):
    """Test cache key generation."""
    project = tmp_path / "test-project"
    project.mkdir()

    (project / "package.json").write_text("{}")

    key1 = generate_cache_key(project, "0.7.0-beta")
    key2 = generate_cache_key(project, "0.7.0-beta")

    # Same key for same project
    assert key1 == key2

    # Different key after file change
    time.sleep(0.1)
    (project / "package.json").write_text('{"name": "test"}')
    key3 = generate_cache_key(project, "0.7.0-beta")
    assert key3 != key1


def test_cache_get_set(tmp_path):
    """Test cache get/set operations."""
    cache = DetectionCache(tmp_path / "cache")

    key = "test_key:abc123:0.7.0-beta"
    result = {
        "framework": "nextjs",
        "version": "14.0.0",
        "language": "typescript",
        "confidence": 1.0,
        "indicators": [],
        "recommended_subagents": [],
        "project_structure": {}
    }

    # Set cache
    cache.set(key, result)

    # Get cache
    entry = cache.get(key)
    assert entry is not None
    assert entry.framework == "nextjs"


def test_cache_expiration(tmp_path):
    """Test TTL-based cache expiration."""
    cache = DetectionCache(tmp_path / "cache")

    key = "test_key:abc123:0.7.0-beta"
    result = {...}

    # Set with 1 second TTL
    cache.set(key, result, ttl=1)

    # Should exist immediately
    assert cache.get(key) is not None

    # Wait for expiration
    time.sleep(1.1)

    # Should be expired
    assert cache.get(key) is None
```

---

## 📈 Success Criteria

**Week 1 Day 5-6 Implementation Complete When**:
- ✅ Cache read/write working
- ✅ Cache key generation tested
- ✅ TTL expiration working
- ✅ File locking implemented
- ✅ CLI flags (--cache, --no-cache) working
- ✅ 20+ unit tests passing
- ✅ Cache hit rate 60%+ in manual testing

**Performance Validation**:
- ✅ Cache hit: <10μs (98% improvement)
- ✅ Cache miss: <460μs (2% overhead acceptable)
- ✅ No crashes with concurrent access

---

**Status**: 🎨 DESIGN COMPLETE
**Next**: Day 2 Afternoon - Test parsing library alternatives
**Implementation**: Day 5-6

**Repository**: https://github.com/SawanoLab/adaptive-claude-agents
**Version**: v0.7.0-beta (Week 1 Day 2)
